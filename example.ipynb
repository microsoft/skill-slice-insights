{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "*** WARNING: THIS EXAMPLE IS INCOMPLETE*** See models.py for how to add new models and pipeline.py for how to get rationales / skills / skill-slices. \n",
    "\n",
    "In this notebook, we will show how to evaluate a new model over our suite of benchmarks, towards performing skill-slice analyses. \n",
    "\n",
    "We will first show how to download the datasets in our initial 12-dataset corpus. \n",
    "\n",
    "Then, we will add a model and evaluate it, via the following steps:\n",
    "1. Implementing the model under our general model class.\n",
    "2. Feeding this model to our standard inference code.\n",
    "3. Retrieving our pre-computed skill slices, and using existing code to compute accuracy per slice.\n",
    "4. Some example analyses that can be done with skill slices, e.g. comparing to frontier models like GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading datasets\n",
    "\n",
    "Our original 12 dataset corpus consists of benchmarks accessible from huggingface, enabling easy downloading. Further, we implement our datasets in a way such that they auto-download lazily (i.e. when first accessed). That is, the first time each instance is accessed, it will be downloaded and saved to the path defined by _DATA_ROOT in constants.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsets import _DSET_DICT\n",
    "from tqdm import tqdm \n",
    "\n",
    "for dsetname in _DSET_DICT:\n",
    "    dset = _DSET_DICT[dsetname]()\n",
    "    for x in tqdm(dset):\n",
    "        continue\n",
    "    print(x)\n",
    "    print(f'Downloaded and saved all instances from {dsetname}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a new model under appropriate model class\n",
    "\n",
    "Each model must implement a `answer_question(question, system_message, image)` method:\n",
    "* Inputs:\n",
    "    * `question`: string, the question / prompt to the model for the specific evaluation instance\n",
    "    * `system_message`: string, the system message specifying the output format. Typically set to `_SYS_MSGS['standard_prompt']` (see constants.py).\n",
    "    * `image`: PIL Image, the image relevant to a given eval instance, or None in the case of a language only query.\n",
    "* Output: string response to the given question\n",
    "\n",
    "It must also have a field called modelname. This name is given to the directory where outputs from the model are cached.\n",
    "\n",
    "We'll now show how to go from a pretrained model hosted on huggingface to something we can feed to existing code in our repo. We begin with the [example inference code provided on huggingface](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct). Then, we move the model and tokenizer to the constructor for our model object, and place inference under `answer_question`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class QwenVL:\n",
    "    def __init__(self):\n",
    "        # default: Load the model on the available device(s)\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        self.modelname = 'Qwen2-VL-7B-Instruct'\n",
    "    \n",
    "    def answer_question(self, question, system_message, image):\n",
    "        ########### THIS IS THE ONLY CODE CHANGE WE NEEDED TO MODIFY THE EXAMPLE INFERENCE CODE ############\n",
    "        if image:\n",
    "            messages = [{\"role\":\"system\", \"content\": [{\"type\":\"text\", \"text\": system_message}]},\n",
    "                    {\"role\": \"user\", \"content\": [{\"type\":\"image\", \"image\": image}, {\"type\":\"text\", \"text\":question}]}]\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "        else:\n",
    "            messages = [{\"role\":\"system\", \"content\": [{\"type\":\"text\", \"text\": system_message}]},\n",
    "                    {\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\":question}]}] \n",
    "            image_inputs, video_inputs = None, None\n",
    "\n",
    "        ### Standard code from huggingface example inference docs: https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and test this code by loading the model and evaluate it on a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QwenVL()\n",
    "dset = _DSET_DICT['mmc']()\n",
    "q = dset[0]\n",
    "print(\"Test Question: \", q[prompt])\n",
    "q['image'].show()\n",
    "\n",
    "ans = model.answer_question(q['prompt'], system_message, q['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Vibhav\n",
    "\n",
    "Here is the code to run verification on the skills annotated by (1) Llama 3.2V 90B, and (2) GPT-4o via direct prompting (instead of rationale parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying skills annotated by open source model (llama 3.2V 90B)\n",
    "First, we have one file to process for Llama 3.2V 90B. So, there is just one function call, after reading in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation import verify_skill_relevance, compute_conf_mats\n",
    "import pandas as pd\n",
    "pos_and_neg_skills = pd.read_csv(FILENAME)\n",
    "_ = verify_skill_relevance(\n",
    "    verifier_model_name='gpt-4o', pos_and_neg_skills=pos_and_neg_skills,\n",
    "    dsetname='mmlu_pro', annotator_model_name='llama32v-chat-90b'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should save the verification results to this path: `os.path.join(_CACHE_ROOT, 'verification', 'results', 'gpt-4o_verifier', 'llama32v-chat-90b_annotator', 'mmlu_pro.csv')`. You can see this path is defined in line 94 of validation.py.\n",
    "\n",
    "To make sure it worked, we can actually check the result of the verification with the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(_CACHE_ROOT, 'verification', 'results', 'gpt-4o_verifier', 'llama32v-chat-90b_annotator', 'mmlu_pro.csv'))\n",
    "cm = compute_conf_mats(df)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we want the values in the diagonals to be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying relevance for direct prompted skills (for prompt ablation)\n",
    "The next thing I need is for you to verify skills for the prompt ablation (i.e., directly asking for skills instead of parsing them from rationales). \n",
    "\n",
    "Here, we have skills for every dataset. I have again created the verification sets, so you just need to run the verification (using gpt-4o). However, now we we must do it for every dataset. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = glob.glob('direct_prompting__verification_sets/*')\n",
    "for f in fs:\n",
    "    pos_and_neg_skills = pd.read_csv(f)\n",
    "    dsetname = f.split('/')[-1].split('.csv')[0]\n",
    "    _ = verify_skill_relevance(\n",
    "        verifier_model_name='gpt-4o', pos_and_neg_skills=pos_and_neg_skills,\n",
    "        dsetname='mmlu_pro', annotator_model_name='direct_prompting_gpt-4o'\n",
    "    )\n",
    "    out_df = pd.read_csv(os.path.join(_CACHE_ROOT, 'verification', 'results', 'gpt-4o_verifier', 'direct_prompting_gpt-4o_annotator', dsetname+'.csv'))\n",
    "    cm = compute_conf_mats(out_df)\n",
    "    print(f\"Dataset: {dsetname}, Confusion matrix:\")\n",
    "    print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
