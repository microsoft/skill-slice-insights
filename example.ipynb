{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "*** WARNING: THIS EXAMPLE IS INCOMPLETE*** See models.py for how to add new models and pipeline.py for how to get rationales / skills / skill-slices. \n",
    "\n",
    "In this notebook, we will show how to evaluate a new model over our suite of benchmarks, towards performing skill-slice analyses. \n",
    "\n",
    "We will first show how to download the datasets in our initial 12-dataset corpus. \n",
    "\n",
    "Then, we will add a model and evaluate it, via the following steps:\n",
    "1. Implementing the model under our general model class.\n",
    "2. Feeding this model to our standard inference code.\n",
    "3. Retrieving our pre-computed skill slices, and using existing code to compute accuracy per slice.\n",
    "4. Some example analyses that can be done with skill slices, e.g. comparing to frontier models like GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading datasets\n",
    "\n",
    "Our original 12 dataset corpus consists of benchmarks accessible from huggingface, enabling easy downloading. Further, we implement our datasets in a way such that they auto-download lazily (i.e. when first accessed). That is, the first time each instance is accessed, it will be downloaded and saved to the path defined by _DATA_ROOT in constants.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsets import _DSET_DICT\n",
    "from tqdm import tqdm \n",
    "\n",
    "for dsetname in _DSET_DICT:\n",
    "    dset = _DSET_DICT[dsetname]()\n",
    "    for x in tqdm(dset):\n",
    "        continue\n",
    "    print(x)\n",
    "    print(f'Downloaded and saved all instances from {dsetname}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a new model under appropriate model class\n",
    "\n",
    "Each model must implement a `answer_question(question, system_message, image)` method:\n",
    "* Inputs:\n",
    "    * `question`: string, the question / prompt to the model for the specific evaluation instance\n",
    "    * `system_message`: string, the system message specifying the output format. Typically set to `_SYS_MSGS['standard_prompt']` (see constants.py).\n",
    "    * `image`: PIL Image, the image relevant to a given eval instance, or None in the case of a language only query.\n",
    "* Output: string response to the given question\n",
    "\n",
    "It must also have a field called modelname. This name is given to the directory where outputs from the model are cached.\n",
    "\n",
    "We'll now show how to go from a pretrained model hosted on huggingface to something we can feed to existing code in our repo. We begin with the [example inference code provided on huggingface](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct). Then, we move the model and tokenizer to the constructor for our model object, and place inference under `answer_question`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class QwenVL:\n",
    "    def __init__(self):\n",
    "        # default: Load the model on the available device(s)\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "        self.modelname = 'Qwen2-VL-7B-Instruct'\n",
    "    \n",
    "    def answer_question(self, question, system_message, image):\n",
    "        ########### THIS IS THE ONLY CODE CHANGE WE NEEDED TO MODIFY THE EXAMPLE INFERENCE CODE ############\n",
    "        if image:\n",
    "            messages = [{\"role\":\"system\", \"content\": [{\"type\":\"text\", \"text\": system_message}]},\n",
    "                    {\"role\": \"user\", \"content\": [{\"type\":\"image\", \"image\": image}, {\"type\":\"text\", \"text\":question}]}]\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "        else:\n",
    "            messages = [{\"role\":\"system\", \"content\": [{\"type\":\"text\", \"text\": system_message}]},\n",
    "                    {\"role\": \"user\", \"content\": [{\"type\":\"text\", \"text\":question}]}] \n",
    "            image_inputs, video_inputs = None, None\n",
    "\n",
    "        ### Standard code from huggingface example inference docs: https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and test this code by loading the model and evaluate it on a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QwenVL()\n",
    "dset = _DSET_DICT['mmc']()\n",
    "q = dset[0]\n",
    "print(\"Test Question: \", q[prompt])\n",
    "q['image'].show()\n",
    "\n",
    "ans = model.answer_question(q['prompt'], system_message, q['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
